---
title: "Cyclistic Bike-Share Case Study"
author: "Udoh Chioma Lilian"
date: "2026-01-28"
output: html_document
---


## 1. Business Task

As a junior data analyst working on the marketing analytics team at Cyclistic, my goal is to understand how casual riders (customers who purchase single-ride or full-day passes) use bikes differently compared to annual members (customers who subscribe annually).

This analysis is intended to be compelling enough for approval by the Cyclistic executive team and will guide the design of an effective marketing strategy. The insights generated will support planning and development of targeted marketing initiatives aimed at converting casual riders into annual members.

Increasing the number of annual memberships is critical to Cyclistic’s future success, as financial analysts have identified membership growth as a key factor in the future growth of the Cyclistic bike-share company.


## 2. Data Source Description and Credibility

This case study uses [Divvy 2019 Q1](https://docs.google.com/spreadsheets/u/0/d/1HyGxbpi5Xad5ARKYb5CQL57LrhR2t_KwCt1CRXus5As/edit) and [Divvy 2020 Q1](https://docs.google.com/spreadsheets/u/0/d/1Y92ILGP-9Wyq418FYuVt14smYzt33G3-T8SoJMCo_UM/edit) datasets, provided by Lyft Bikes and Scooters, LLC (“bikeshare”), a bike-share company operating in Chicago. The data is made available under a public [license](https://divvybikes.com/data-license-agreement) by Motivate International Inc., granting permission to access, analyze, and use the data for lawful purposes.

The datasets are structured in Excel format with rows representing individual trips and columns describing trip attributes. For the purpose of this case study, the datasets are appropriate, reliable and comprehensive, containing all necessary variables required for this analysis. Although the data is not fully current, it is appropriate for identifying usage patterns and behavioral differences between rider types.

Personally identifiable information has been removed, ensuring rider privacy. The data was handled securely on a password-protected device and used strictly for analytical purposes.

## 3. Data Cleaning and Preparation

```{r setup, include=FALSE}
library(dplyr)
library(stringr)
library(janitor)
library(lubridate)
library(ggplot2)
library(DT)
library(scales)
library(tidyverse)
library(readxl)
```

```{r load-data, include=FALSE}
library(readxl)

# Load the 2019 data
Divvy_Trips_2019_Q1 <- read_excel("C:/Users/HP/Downloads/Divvy_Trips_2019_Q1.xlsx")

# Load the 2020 data
Divvy_Trips_2020_Q1 <- read_excel("C:/Users/HP/Downloads/Divvy_Trips_2020_Q1.xlsx")

```

```{r data-cleaning, include=FALSE}
library(stringr)
library(janitor)
library(tidyverse)
library(lubridate)
library(ggplot2)
library(scales)
library(readxl)

  library(stringr)
  library(janitor)
  library(tidyverse)
  
  #To check for duplicate trip_id
  Divvy_Trips_2019_Q1 %>%
    count(trip_id) %>%
    filter(n > 1)
  #There are no duplicates
  
  
  #To ensure no spaces in column names and all column names are in lowercase
  clean_names(Divvy_Trips_2019_Q1) 
  
  
  #To view the new cleaned dataframe
  clean1_Divvy_Trips_2019_Q1 <- clean_names(Divvy_Trips_2019_Q1)
  
  
  #To count the number of rows with empty cells
  Divvy_Trips_2019_Q1 %>%
    filter(if_any(everything(), is.na)) %>%
    nrow()
  #There are 19712 rows
  
  
  #To delete empty cells
  clean2_Divvy_Trips_2019_Q1 <- drop_na(clean1_Divvy_Trips_2019_Q1)
  
  
  # To detect inconsistent suffixes using the full name of observed abreviations prominent in the dataframe
  patterns <- c("\\bRoad\\b", "\\bStreet\\b", "\\bParkway\\b", "\\bAvenue\\b",
                "\\bBoulevard\\b", "\\bPlace\\b", "\\bDrive\\b")
  
  
  # Combine into one regex
  regex_pattern <- paste(patterns, collapse = "|")
  
  
  # Filter rows where either start or end station has any of these exact patterns
  clean3_Divvy_Trips_2019_Q1 <- clean2_Divvy_Trips_2019_Q1 %>%
    filter(
      str_detect(from_station_name, regex_pattern) |
        str_detect(to_station_name, regex_pattern)
    )
  #Inconsistency detected in place appearing as pl in most places and place in a certain station name, Mccormick Place
  
  
  #Make the naming format consistent as "Pl" using global substitution
  clean4_Divvy_Trips_2019_Q1 <- clean2_Divvy_Trips_2019_Q1
  clean4_Divvy_Trips_2019_Q1$from_station_name <- 
  gsub("Place", "Pl", clean2_Divvy_Trips_2019_Q1$from_station_name)
  
  
  clean5_Divvy_Trips_2019_Q1 <- clean4_Divvy_Trips_2019_Q1
  clean5_Divvy_Trips_2019_Q1$to_station_name <-
  gsub("Place", "Pl", clean4_Divvy_Trips_2019_Q1$to_station_name)
  
  
  #To confirm fixed inconsistent suffixes
  patterns <- c("\\bPlace\\b")
  
  
  # Combine into one regex
  regex_pattern <- paste(patterns, collapse = "|")
  
  
  # Filter rows where either start or end station has any of these exact patterns
  clean5_Divvy_Trips_2019_Q1 %>%
    filter(
      str_detect(from_station_name, regex_pattern) |
        str_detect(to_station_name, regex_pattern)
    ) %>%
    select(from_station_name, to_station_name)
  #result is empty meaning it has been consistently formatted.
  
  
  clean6_Divvy_Trips_2019_Q1 <- clean5_Divvy_Trips_2019_Q1 %>%
    mutate(across(where(is.character), ~ str_to_title(str_squish(trimws(str_remove_all(., "\\(.*?\\)"))))))
  #To remove any leading or trailing spaces and remove the Asterisk appearing in some street address across all addresses.
  #library(dplyr) loads dplyr, which gives you functions like mutate(), across(), %>% (the pipe).
  #library(stringr) loads stringr, which has handy string functions like str_squish().
  #cleaned_Divvy_Trips_2019_data %>% ... the %>%  takes what you have written in the first code line and calls information from it with the next one
  #mutate(...) is saying “create new columns or change existing ones.”
  #across(where(is.character), ~ str_squish(trimws(.)))
  #across lets you apply a function to multiple columns at once rather than having to call out each columns one after the other.
  #where(is.character) picks all columns that are text/strings.
  #~ str_squish(...) implies for each cell in those columns:
  #str_to_title sets texts to title case
    #trimws(.) removes spaces at the start or end of the text
  #str_squish(.)removes extra spaces inside the text, leaving just one space between words
  
  
  #Rename from_station_name to start_station_name
  #Rename to_station_name to end_station_name
  #Rename from_station_id to start_station_id
  #Rename to_station_name to end_station_id
  
  
  clean6_Divvy_Trips_2019_Q1 <- clean6_Divvy_Trips_2019_Q1 %>%
    rename(start_station_name = from_station_name, 
           end_station_name = to_station_name,
           start_station_id = from_station_id, 
           end_station_id = to_station_id)
  
  #Longitude and latitude values were validated by confirming that latitude values
  # fell within the expected range (−90 to 90) and longitude values within (−180 to 180).
  # The observed negative longitude values are consistent with the study area being 
  #located west of the Prime Meridian.”
  clean7_Divvy_Trips_2019_Q1 <- clean6_Divvy_Trips_2019_Q1%>%
    select(-gender,-birthyear, -bikeid)
  
  
  #To confirm that trip_duration is stored in seconds
  tripduration_check <- clean7_Divvy_Trips_2019_Q1 %>%
    mutate(tripduration_seconds = as.numeric(difftime(end_time, start_time, units = "secs")))
  
  
  #To confirm that their calculation is correct
  tripduration_check <- tripduration_check %>%
    mutate(duration_diff = tripduration - tripduration_seconds)
  
  tripduration_check %>%
    filter(duration_diff != 0) %>%
    nrow()
  #16 diferences noted, Hence we use our calculated field
  
  
  clean8_Divvy_Trips_2019_Q1 <- tripduration_check%>%
    select(-tripduration, - duration_diff)
  
  
  
  
  #for divvy_trips_2020
  #To check for duplicate ride_id
  Divvy_Trips_2020_Q1 %>%
    count(ride_id)%>%
    filter(n>1)
  
  
  #To check through the dataframe and count how many rows have at least one empty cell
  Divvy_Trips_2020_Q1%>%
    filter(if_any(everything(), is.na))%>%
    nrow()
  
  
  #To delete empty cells
  clean1_Divvy_Trips_2020_Q1 <- drop_na(Divvy_Trips_2020_Q1)
  
  
  #To remove any leading or trailing spaces and remove the observed Asterisk appearing in some street address across all addresses.
  clean2_Divvy_Trips_2020_Q1 <- clean1_Divvy_Trips_2020_Q1%>%
    mutate(across(where(is.character), ~str_to_title(str_squish(trimws(str_remove_all(., "\\(.*?\\)"))))))
  
  
  #To ensure that there are only characters, letters and underscores in the dataframe
  clean3_Divvy_Trips_2020_Q1 <- clean_names(clean2_Divvy_Trips_2020_Q1)
  
  
  
  # To detect inconsistent suffixes using the full name of observed abreviations prominent in the dataframe
  patterns <- c("\\bRoad\\b", "\\bStreet\\b", "\\bParkway\\b", "\\bAvenue\\b",
                "\\bBoulevard\\b", "\\bPlace\\b", "\\bDrive\\b")
  
  # Combine into one regex
  regex_pattern <- paste(patterns, collapse = "|")
  
  # Filter rows where either start or end station has any of these exact patterns
  clean3_Divvy_Trips_2020_Q1 %>%
    filter(
      str_detect(start_station_name, regex_pattern) |
        str_detect(end_station_name, regex_pattern)
    ) %>%
    select(start_station_name, end_station_name)
  
  
  #This shows that place was formatted inconsistently as pl in most places and as place in some others. The others didn't appear in the dataframe
  #This shows 565 rows
  
  
  #Rectify this by using global substitution, gsub()
  
  
  clean4_Divvy_Trips_2020_Q1 <- clean3_Divvy_Trips_2020_Q1
  clean4_Divvy_Trips_2020_Q1$start_station_name <-
    gsub("Place", "Pl", clean3_Divvy_Trips_2020_Q1$start_station_name)
  
  clean5_Divvy_Trips_2020_Q1 <- clean4_Divvy_Trips_2020_Q1
  clean5_Divvy_Trips_2020_Q1$end_station_name <-
    gsub("Place", "Pl", clean3_Divvy_Trips_2020_Q1$end_station_name)
  
  #To confirm fixed inconsistent suffixes
  patterns <- c("\\bPlace\\b")
  
  
  # Combine into one regex
  regex_pattern <- paste(patterns, collapse = "|")
  
  
  # Filter rows where either start or end station has any of these exact patterns
  clean5_Divvy_Trips_2020_Q1 %>%
    filter(
      str_detect(start_station_name, regex_pattern) |
        str_detect(end_station_name, regex_pattern)
    ) %>%
    select(start_station_name, end_station_name)
  #result is empty meaning it has been consistently formatted.
  
  #Longitude and latitude values were validated by confirming that latitude values
  # fell within the expected range (−90 to 90) and longitude values within (−180 to 180).
  # The observed negative longitude values are consistent with the study area being 
  #located west of the Prime Meridian.”
  
  #Rename started_at to start_time and ended_at to end_time
  
  clean6_Divvy_Trips_2020_Q1 <- clean5_Divvy_Trips_2020_Q1 %>%
    rename(start_time = started_at, 
           end_time = ended_at,
           usertype = member_casual,
           trip_id = ride_id)
  
  #Remove columns for merging purpose
  clean7_Divvy_Trips_2020_Q1 <- clean6_Divvy_Trips_2020_Q1%>%
    select(-rideable_type, -start_lat, -start_lng, -end_lat, -end_lng)
  
  clean8_Divvy_Trips_2020_Q1 <- clean7_Divvy_Trips_2020_Q1 %>%
    mutate(tripduration_seconds = as.numeric(difftime(end_time, start_time, units = "secs")))
  
  
  #To make both datatypes character_datatype 
  clean8_Divvy_Trips_2019_Q1 <- clean8_Divvy_Trips_2019_Q1 %>%
    mutate(trip_id = as.character(trip_id))
  
  clean8_Divvy_Trips_2020_Q1 <- clean8_Divvy_Trips_2020_Q1 %>%
    mutate(trip_id = as.character(trip_id))
  
  
  merged_divvy_data1 <-
  bind_rows(clean8_Divvy_Trips_2019_Q1, clean8_Divvy_Trips_2020_Q1)
  
  
  #To change name of cell from customer to casual and from subscribers to members
  merged_divvy_data2 <- merged_divvy_data1 %>%
    mutate(usertype = recode(usertype,
                             "Customer" = "Casual",
                             "Subscriber" = "Member"))
  
  
  #To rearrange columns
  merged_divvy_data3 <- merged_divvy_data2%>%
    relocate(tripduration_seconds, .before = start_station_id)
  
  
  #Add day of week for the start_time and end_time
  library(lubridate)
  
  merged_divvy_data4 <- merged_divvy_data3 %>%
    mutate(
      start_day_of_the_week = wday(start_time, label = TRUE),
      end_day_of_the_week   = wday(end_time,   label = TRUE)
    )
  
  
  #I arranged rows in descending order to check for false tripduration_seconds 
  #and spoted some -ve values and some vakues less thn 60s which i use as my bech benchmark for rides
  #I also noticed some data with aoutrageous amounts of time spent on rides when u arranged by clicking on the 
  #arro in the r sorce(aspa that place that shows the satasets) and so i set a benc hmark to emove all greater than 3 hrs tha is 10800seconds
  
  Original_merged_rows <- nrow(merged_divvy_data4)
  
  
  merged_divvy_data5 <- merged_divvy_data4 %>%
    filter(tripduration_seconds > 0) %>%   # remove negative durations
    filter(tripduration_seconds >= 60) %>%    # remove very short trips
    filter(tripduration_seconds <= 10800)
  
  
  cleaned_merged_rows <- nrow(merged_divvy_data5)
  
  removed_merged_rows <- Original_merged_rows - cleaned_merged_rows
  
  removed_percent <- (removed_merged_rows / Original_merged_rows) * 100
  #1.238% of the data  
  
  #Rearranged datasets again
  merged_divvy_data6 <- merged_divvy_data5%>%
    relocate(start_day_of_the_week, end_day_of_the_week, .before = start_station_id)
  
  
  #ANALYSES PHASE (I GUESS...LOL)
  
  #mean and median trip duration for each usertyes
  summary_tripduration <- merged_divvy_data6 %>%
    group_by(usertype) %>%
    summarise(
      avg_tripduration_secs   = mean(tripduration_seconds),
      median_tripduration_secs = median(tripduration_seconds)
    )
  #in minutes
  summary_tripduration %>%
    mutate(avg_tripduration_mins = avg_tripduration_secs / 60,
           median_tripduration_mins = median_tripduration_secs / 60)%>%
    select(-avg_tripduration_secs, -median_tripduration_secs)
  
  #The avg_trip duration for casual users was 31minutes and from member was 11 min
  #This was closely related to the median values of 22.3 minutes for casual users and 
  #8.48 minutes for members.
  #This implies that casual riders use bike for a longer duration than members
  
  
  #We now dive deeper to observe how these two users differs by days of the week
  #I realised i still had two columns and decided to work with a sinlge colium for dow but then 
  #going through my data i realised that some hrs crossed over from one day into another and so
  #I picked the start day as my day of week and proceeded to exploring the differement bewteen users by dow
  
  
  all(merged_divvy_data6$start_day_of_the_week ==
        merged_divvy_data6$end_day_of_the_week)
  
  merged_divvy_data6 %>%
    filter(start_day_of_the_week != end_day_of_the_week) %>%
    nrow()
  
  merged_divvy_data6 %>%
    filter(start_day_of_the_week != end_day_of_the_week) %>%
    select(start_time, end_time, start_day_of_the_week, end_day_of_the_week)
  
  
  #To remove end day of week and change start day to trip_day
  merged_divvy_data7 <- merged_divvy_data6 %>%
    select(- end_day_of_the_week)%>%
    rename(trip_day = start_day_of_the_week )
  
  #Compare the difference in usage of bikesfor each usertype
  daily_summary_casual <- merged_divvy_data7 %>%
    filter(usertype == "Casual") %>%
    group_by(trip_day) %>%
    summarise(
      avg_tripduration_min = mean(tripduration_seconds/60),
      median_tripduration_min = median(tripduration_seconds/60),
      .groups = "drop"
    )
  
  daily_summary_member <- merged_divvy_data7 %>%
    filter(usertype == "Member") %>%
    group_by(trip_day) %>%
    summarise(
      avg_tripduration_min = mean(tripduration_seconds)/60,
      median_tripduration_min = median(tripduration_seconds/60),
      .groups = "drop"
    )
  
  daily_summary <- merged_divvy_data7 %>%
    group_by(usertype, trip_day) %>%
    summarise(avg_tripduration = mean(tripduration_seconds) / 60)
  
  ggplot(daily_summary, aes(trip_day, avg_tripduration, fill = usertype)) +
    geom_col(position = "dodge") +
    labs(title = "Average Trip Duration by Day of Week",
         x = "Day of Week",
         y = "Average Trip Duration (minutes)")
  
  #Casual riders consistently take longer trips across all days, 
  #with peaks on weekends. Members show stable, shorter ride durations during weekdays,
  #reinforcing commuter behavior.
  #The highest average trip duration for both casual and members were on sundays. 
  #For both groups, the mean is greater than the median showing that tthe datsa is  slighlty skwed.
  #Casual users consistently took longer trips than members, with average trip durations 
  #peaking on Sundays. Median values show a similar pattern, 
  #confirming that casual users’ rides are both longer and slightly more variable.
  #for Lets see if this will 
  #be same for the count per day to see which day bike is being used the most
 
  
  
  
  
  #daiy trip
  daily_trip_counts <- merged_divvy_data7 %>%
    group_by(usertype, trip_day) %>%
    summarise(
      trip_count = n(),
      .groups = "drop"
    )
  
  
  #Add a column for day_type to calculate avg count by day of the week
  merged_divvy_data8 <- merged_divvy_data7 %>%
    mutate(
      day_type = ifelse(trip_day %in% c("Sat", "Sun"),
                        "Weekend", "Weekday"))%>%
    relocate(day_type, .after = trip_day)
  
  #avg trip count by daytype
  avg_day_type_trip_count <- merged_divvy_data8 %>%
    group_by(usertype, trip_day, day_type) %>%
    summarise(daily_trip_count = n(), .groups = "drop") %>%
    group_by(usertype, day_type) %>%
    summarise(
      avg_daily_trip_count = mean(daily_trip_count),
      .groups = "drop"
    )
  
  
  library(ggplot2)
  library(scales)
  
  ggplot(daily_trip_counts,
         aes(x = trip_day,
             y = trip_count,
             fill = usertype)) +
    geom_col(position = "dodge") +
    scale_y_continuous(labels = comma) +
    labs(
      title = "Daily Trip Count by Day of the Week",
      x = "Day of the Week",
      y = "Number of Trips",
      fill = "User Type"
    ) +
    theme_minimal()
  
  

  
  # Plot the average trip counts
  ggplot(avg_day_type_trip_count, aes(x = day_type, y = avg_daily_trip_count, fill = usertype)) +
    geom_col(position = "dodge") +
    labs(
      title = "Average Daily Trip Count by User Type: Weekday vs Weekend",
      x = "Day Type",
      y = "Average Number of Trips",
      fill = "User Type"
    ) +
    theme_minimal()
  
  
  combined_summary <- merged_divvy_data8 %>%
    group_by(usertype, day_type) %>%
    summarise(
      avg_trip_count = n() / n_distinct(trip_day),
      avg_trip_duration_min = mean(tripduration_seconds) / 60,
      .groups = "drop"
    )
  
  combined_summary
  
  #Rearranged the dataset for plotting purposes
  combined_long <- combined_summary %>%
    pivot_longer(
      cols = c(avg_trip_count, avg_trip_duration_min),
      names_to = "metric",
      values_to = "value"
    )
  
  combined_long
  
  
  #plot
  library(ggplot2)
  library(scales)
  
  ggplot(combined_long,
         aes(x = day_type, y = value, fill = usertype)) +
    geom_col(position = "dodge") +
    facet_wrap(~ metric, scales = "free_y",
               labeller = as_labeller(c(
                 avg_trip_count = "Average Trip Count",
                 avg_trip_duration_min = "Average Trip Duration (Minutes)"
               ))) +
    scale_y_continuous(labels = comma) +
    labs(
      title = "Trip Frequency and Duration by User Type and Day Type",
      x = "Day Type",
      y = "Value",
      fill = "User Type"
    ) +
    theme_minimal()
 #Casual riders use bikes the mostly on weekends with the amount of times bokes 
  #where order on satand subn as 15507 and 8582 times.There was a huge drop on weekdays 
  #Members use bikes at a high count rate on weeddays, even about 7 times more than the caual iridders use it on weekends
  #On average no of bikes picked up by members on weeekdays are 119046 bikes and 59059 on weeekends
  #While avg  count of bikes casual users is  12044 on weekend and , 5047 on weeekdays
  #we observed that bikes are used mostly bike members tha by casual riders. 
  #While long trips are made by casual users they dont use bikes as much as often as the members
  #who use it very often but take shorter trip in term of trip length
  
  
  #We now dive deeper  to see which hours for each users , which hour  users chose to ride
  #i.e which is the most recorded hrs for the start_time for each users
  
  merged_divvy_data9 <- merged_divvy_data8 %>%
    mutate(start_hour = hour(start_time)) %>%
    relocate(start_hour, .before = trip_day)

  
  #see trip count  by start hour
  trips_by_hour_overall <- merged_divvy_data9 %>%
    group_by(start_hour) %>%
    summarise(trip_count = n()) %>%
    arrange(start_hour)

  
  #Overall , most trips occurs between 7am - 8 am and 4pm -5pm . The east occrs a 3ami predict that the
  #members might be skewing this dataset in terms of when most rides ocur so we dive deper to seeing if this is so for each user
  #To see if theres a  difference in hourly behaviour 
  
  
  #trip Count by usertype and start hour o
  
  hourly_trip_by_usertype <- merged_divvy_data9 %>%
    group_by(usertype, start_hour) %>%
    summarise(trip_count = n(), .groups = "drop") %>%
    arrange(usertype, desc(trip_count))
  #Casual users peak around 13–17 (1 PM–5 PM) → afternoon/early evening
  #casual riders raraely ride at 0-5(12am-5am) that is at night / very early morning
  #Members peak around 7–8am and again around 4-6pm → mostly daytime commuter hours
  #late morning 9am and midday rides 12-3 exists but are not as intense at commuting hours
  #members similarly to cusal rideers ride least at night from 0-4 am with a slighly higher amunt from 
  
  
  
  #we now see if theres stations that are popular for members and for casual riders
  # Top start stations overall
  top_start_stations <- merged_divvy_data9 %>%
    group_by(start_station_name) %>%        # Group by start station
    summarise(station_count = n()) %>%        # Count trips per station
    arrange(desc(station_count))              # Sort descending

  
  top_start_stations_usertype <- merged_divvy_data9 %>%
    group_by(usertype, start_station_name) %>% # Group by user type + start station
    summarise(station_count = n()) %>%
    slice_max(station_count, n = 10)%>%
    arrange(usertype, desc(station_count)) 
  
  #we now see if theres stations that are popular for members and for casual riders
  # Top start stations overall
  top_end_stations <- merged_divvy_data9 %>%
    group_by(end_station_name) %>%        # Group by start station
    summarise(station_count = n()) %>%        # Count trips per station
    arrange(desc(station_count))              # Sort descending
 
  
  
  top_end_stations_usertype <- merged_divvy_data9 %>%
    group_by(usertype, end_station_name) %>% # Group by user type + start station
    summarise(station_count = n()) %>%
    slice_max(station_count, n = 10)%>%
    arrange(usertype, desc(station_count)) 
```


### 3.1 Cleaning Divvy 2019 Q1 Data

cleaning steps included:

* Checking and confirming absence of duplicate `trip_id` values
* Standardizing column names to lowercase with no spaces
* Removing rows with missing values
* Resolving inconsistent station name suffixes (e.g., "Place" to "Pl")
* Removing leading and trailing whitespacesacross all entries
* Removoing asterisks present in some street address entries across all addresses
* Renaming columns for consistency with 2020 data
* Validating latitude and longitude ranges
* Recalculating trip duration in seconds using start and end times

Trips with incorrect or inconsistent duration values were corrected using calculated durations.

### 3.2 Cleaning Divvy 2020 Q1 Data

Cleaning steps mirrored the 2019 process and additionally included:

* Renaming columns to align with the 2019 schema
* Removing unnecessary columns to enable merging
* Creating a calculated trip duration field

### 3.3 Data Merging and Filtering

The cleaned 2019 and 2020 datasets were merged into a single dataset. User types were standardized as "Casual" and "Member". Additional cleaning included:

* Creating start and end day of the week columns
* Rearraging tripduration_seconds field to identify any anomalies that could exist
* Removing negative trip durations
* Excluding trips shorter than 60 seconds
* Removing trips longer than 3 hours (10,800 seconds)

Only 1.238% of the total data was removed, ensuring minimal data loss while improving data quality. I now rearranged my datasets so that related fields are positioned close together for better readability and logical flow.

```{r merged_divvy_data6, echo=FALSE, message=FALSE, warning=FALSE}
library(DT)

# Show only the first 100 rows
datatable(
  head(merged_divvy_data6, 100),
  options = list(
    pageLength = 10,        # shows 10 rows per page
    scrollX = TRUE,         # horizontal scroll for wide tables
    lengthMenu = c(10, 25, 50),
    autoWidth = TRUE
  ),
  filter = 'top',
  class = 'cell-border stripe hover'
)


```

## 4. Analysis

### 4.1 Trip Duration Comparison


```{r trip-duration-summary}
summary_tripduration <- merged_divvy_data6 %>%
  group_by(usertype) %>%
  summarise(
    avg_tripduration_mins = mean(tripduration_seconds) / 60,
    median_tripduration_mins = median(tripduration_seconds) / 60
  )
summary_tripduration
```

**Insight:** Casual riders take significantly longer trips (average ≈ 31 minutes) compared to members (average ≈ 11 minutes), which is reflected in the median values of 22.3 minutes for casual riders and 8.48 minutes for members. Next, we examine how their riding patterns vary across the days of the week.

### 4.2 Comparing Usage by Day of the Week

```{r}
daily_summary <- merged_divvy_data7 %>%
  group_by(usertype, trip_day) %>%
  summarise(avg_tripduration = mean(tripduration_seconds) / 60)

ggplot(daily_summary, aes(trip_day, avg_tripduration, fill = usertype)) +
  geom_col(position = "dodge") +
  labs(title = "Average Trip Duration by Day of Week",
       x = "Day of Week",
       y = "Average Trip Duration (minutes)")
```

**Insight:** Casual riders consistently take longer trips across all days, with peaks on weekends. These notable peaks could suggest that they are leisure-oriented. Members show stable, shorter ride duration during weekdays, similar to the behavior of commuters. The highest average trip duration for both casual riders and members was on Sundays. While this hints at the types of users, we can gain more insight by examining the daily trip counts to see which daytype(weekdays and weekends) bikes are used the most. This will help guide our analysis toward strategies for converting casual riders into members.

### 4.3 Comparing Trip Counts: Weekday vs Weekend

To better understand how casual riders and members use bikes across the days of the week. 
Next, I added a column `day_type` to distinguish between weekdays and weekends. This allows us to calculate the average number of trips taken by each user type on weekdays versus weekends. 

```{r daily_trip_count}
library(ggplot2)
library(scales)

ggplot(daily_trip_counts,
       aes(x = trip_day,
           y = trip_count,
           fill = usertype)) +
  geom_col(position = "dodge") +
  scale_y_continuous(labels = comma) +
  labs(
    title = "Daily Trip Count by Day of the Week",
    x = "Day of the Week",
    y = "Number of Trips",
    fill = "User Type"
  ) +
  theme_minimal()

```


```{r avg_daytype_trip_count}
avg_day_type_trip_count <- merged_divvy_data8 %>%
  group_by(usertype, day_type) %>%
  summarise(avg_daily_trips = mean(n()))

ggplot(avg_day_type_trip_count, aes(day_type, avg_daily_trips, fill = usertype)) +
  geom_col(position = "dodge") +
  labs(title = "Average Daily Trips by User Type",
       x = "Day Type",
       y = "Average Trip Count")
```


To make a sense of it all we see a table showing the average daytype trip count and the average trip duration by daaytype to see tehe diference in count of trips and duration of trips between the two groups

```{r}
 combined_summary <- merged_divvy_data8 %>%
    group_by(usertype, day_type) %>%
    summarise(
      avg_trip_count = n() / n_distinct(trip_day),
      avg_trip_duration_min = mean(tripduration_seconds) / 60,
      .groups = "drop"
    )
  
  combined_summary
```


```{r trip frequency and duration}
 #plot
  library(ggplot2)
  library(scales)
  
  ggplot(combined_long,
         aes(x = day_type, y = value, fill = usertype)) +
    geom_col(position = "dodge") +
    facet_wrap(~ metric, scales = "free_y",
               labeller = as_labeller(c(
                 avg_trip_count = "Average Trip Count",
                 avg_trip_duration_min = "Average Trip Duration (Minutes)"
               ))) +
    scale_y_continuous(labels = comma) +
    labs(
      title = "Trip Frequency and Duration by User Type and Day Type",
      x = "Day Type",
      y = "Value",
      fill = "User Type"
    ) +
    theme_minimal()
```

**Insight:** For casual users, the average number of trips is about 12,044 on weekends and 5,047 on weekdays. Members, on the other hand, use bikes at a much higher rate on weekdays, recording about seven times more trips than casual riders do on weekends.Their average number of trips is approximately 119,046 trips on weekdays and 59,059 trips on weekends.
This comparison highlights a clear behavioral difference between casual riders and members. Members record significantly higher trip counts, particularly on weekdays, suggesting frequent and routine usage consistent with commuting behavior. In contrast, casual riders show lower trip frequencies, especially on weekdays, but exhibit longer average trip durations, particularly during weekends. This indicates that while casual riders use the service less often, they tend to engage in longer leisure-oriented rides when they do use the bikes.

Next, we explore to see which time of the day the bikes aare used the most to further see the behaviuoural difference beytween thesis users.

### 4.4 Hourly Ride Patterns

```{r hourly-patterns}
hourly_trips <- merged_divvy_data9 %>%
  group_by(usertype, start_hour) %>%
  summarise(trip_count = n())

ggplot(hourly_trips, aes(start_hour, trip_count, color = usertype)) +
  geom_line() +
  labs(title = "Hourly Ride Patterns",
       x = "Hour of Day",
       y = "Trip Count")
```

**Insight:** Members peak during morning and evening commute hours (7–9 AM, 4–6 PM), while casual riders peak in the afternoon, again, indicating leisure-based usage.

### 4.5 Popular Stations

```{r top-stations}
top_start_stations_usertype %>%
  ggplot(aes(reorder(start_station_name, station_count), station_count, fill = usertype)) +
  geom_col() +
  coord_flip() +
  labs(title = "Top Start Stations by User Type",
       x = "Station",
       y = "Trip Count")
```

**Insight:** Members tend to start trips from central, commuter-friendly locations, while casual riders favor recreational or tourist-heavy stations.

## 5. Key Findings

* Casual riders take longer but fewer trips, particularly on weekends
* Members ride more frequently with shorter durations, especially on weekdays
* Members’ usage patterns align with commuting behaviour
* Casual riders’ behavior aligns with leisure and recreational use

## 6. Recommendations

#### **Personalised and simplified data communication**
Cyclistic can analyse riders’ historical trip data to provide simple summaries showing:
- How long casual riders typically ride  
- How much they spend on longer trips  
- How this compares with what they would pay under a membership  

Presenting this information in a clear and non-technical format can help casual riders better understand their own usage patterns. This can also help Cyclistic decide how and when to communicate with these users. From the analysis, Cyclistic can use:
- In-app notifications  
- App pop-ups after a completed ride  
- Email summaries  

These methods are cheaper, less aggressive, and more likely to be read at the right time.


#### **Encourage a one-month trial membership (not annual first)**
Since many casual riders believe they “don’t ride enough to sign up as an annual member,” Cyclistic should offer a one-month membership trial through in-app pop-ups and email notifications. Messages such as *“Still doubting? Try it for one month and see for yourself”* can help reduce hesitation.

This lets users test the value of membership themselves. After the one-month trial, Cyclistic can show users a comparison of:
- What they actually spent during the month  
- What they would have spent without membership  

This helps riders compare spending and decide based on real experience.


#### **Offer a discounted annual membership after the trial**
For riders who complete the trial period, Cyclistic can offer a discounted annual membership. This offer should be presented as a reward rather than pressure. By targeting users who have already experienced the value of membership, conversion becomes more realistic and sustainable.

#### **Target casual riders at the right time and place**
Since casual riders mostly ride on weekends and during specific daytime hours, Cyclistic can promote membership through:
- In-app notifications during peak casual riding hours  
- Posters and QR codes, at high-traffic casual start stations  
- App notifications shown near ride start and after ride completion  
This ensures that marketing efforts reach casual riders when they are already thinking about using the bikes.

## 7. Conclusion

Casual and member riders use Cyclistic bikes in fundamentally different ways. By leveraging these behavioral insights, Cyclistic can design targeted and realistic strategies to convert casual riders into loyal annual members, ultimately driving sustainable growth and revenue stability.



